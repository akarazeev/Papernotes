RL предоставляет подход, который решает проблему обучения агента выбору оптимальных действий для достижения целей (сам агент "чувствует" и действует в своём окружении).

Архитектура MQ-Trader состоит из четырех объединенных Q-learning агентов: первые два - `buy and sell signal agents` (определяют моменты купли и продажи), остальные два - `buy and sell order agents` (определяют best buy price (`BP`) and sell price (`SP`)).

- `MAs` stands for "moving averages"
- `TP` stands for "turning point" matrix

#### A. Proposed Learning Framework

1. signal agent
1. order agent

Дан рандомный stock item. Этап обучения начинается с выбора рандомного дня из истории окружения (обозначим его за delta).

![Fig.1](images/qlearning-for-stocktrading_1.png)

`Buy signal agent` предсказывает цену, анализируя последние изменения цены на рассматриваемой бирже, и принимает решение купить акцию, если цена скорее всего пойдет вверх в ближайшее время. Если решает не покупать, то этап заканчивается. И всё начинается заново с рандомного дня.

Всю последовательность действий агентов можно видеть на `Fig. 1`.

После покупки в день `delta + 1` sell signal agent проверяет историю цен купленной акции, а также текущую накопленную прибыль/потерю, чтобы решить - продавать акцию или удерживать её.

Если sell signal agent решает удержать акцию в день delta + k (step 6b), окружение награждает его и обновленное состояние, чтобы этот же процесс мог повториться на следующий день.

![Fig.2](images/qlearning-for-stocktrading_2.png)

Количество дней, которые sell signal agent может удерживать акцию, ограничено.

#### B. State Representations for Signal Agents

Одна из главных задач - представление состояния агента.

Предлагается схема, которая отображает состояние, называется `TP matrix` (также она сжато учитывает изменение цен за длительный период).

#### State Representations for Order Agents



III. LEARNING ALGORITHMS FOR MQ-TRADER AGENTS
